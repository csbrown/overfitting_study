{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os.path\n",
    "from cifar10_models import *\n",
    "from cifar10_input import *\n",
    "from utils import *\n",
    "import itertools\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# get TF logger\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler('tensorflow.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)\n",
    "\n",
    "#### SET THIS BEFORE RUNNING\n",
    "DIR = \"cifar10\"\n",
    "NAME = \"cifar10_scm_{}_{}_{}_{}\"\n",
    "PATH = os.path.join(DIR,NAME)\n",
    "\n",
    "def train(model, name, restore_point = None, wd1=0., wd2=0., wd3=0., drop1=1., save=True):\n",
    "    with tf.Session() as sess:\n",
    "        model._restore_or_init(sess, restore_point)\n",
    "        model.weight_decay_layer_1.assign(wd1)\n",
    "        model.weight_decay_layer_2.assign(wd2)\n",
    "        model.weight_decay_layer_3.assign(wd3)\n",
    "        model.drop_fc1.assign(drop1)\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                sess.run(model.train_step)\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            pass\n",
    "        \n",
    "        global_step = model.global_step.eval()\n",
    "        if save:\n",
    "            checkpoint_name = CHECKPOINT.format(name) + \"_\" + str(global_step).rjust(10,\"0\")\n",
    "            tf.logging.info(\"Saving to: \" + checkpoint_name)\n",
    "            model.saver.save(sess, checkpoint_name)\n",
    "    return checkpoint_name\n",
    "        \n",
    "def evaluate(model, restore_point = None):\n",
    "    acc = []\n",
    "    with tf.Session() as sess:\n",
    "        model._restore_or_init(sess, restore_point)\n",
    "        try:\n",
    "            while True:\n",
    "                acc.append(model.accuracy.eval())\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            pass\n",
    "    return np.mean(acc)\n",
    "\n",
    "@np.vectorize\n",
    "def nplte(x,y):\n",
    "    return x<=y\n",
    "\n",
    "def train_to_convergence(train_files, wd1=0., wd2=0., wd3=0., drop1=1., save_rate=10):\n",
    "    acc = []\n",
    "    checkpoint = None\n",
    "    name = PATH.format(wd1,wd2,wd3,drop1)\n",
    "    make_dirs(CHECKPOINT.format(name))\n",
    "    i = 0\n",
    "    while len(acc)<30 or nplte(np.array(acc[-20:]) - np.array(acc[-21:-1]),0).sum() < 13: # until the accuracy is just as likely to go down as up\n",
    "        tf.reset_default_graph()\n",
    "        train_data = Cifar10Record(train_files, epochs=1)\n",
    "        model = Cifar10ShallowConvolutionalModel(train_data.image, train_data.label)\n",
    "        save = not i%save_rate\n",
    "        checkpoint = train(model, name, restore_point = checkpoint, wd1=wd1, wd2=wd2, wd3=wd3, drop1=drop1, save=save)\n",
    "        tf.reset_default_graph()\n",
    "        train_data = Cifar10Record(train_files, epochs=1)\n",
    "        model = Cifar10ShallowConvolutionalModel(train_data.image, train_data.label)\n",
    "        acc.append(evaluate(model, restore_point = checkpoint))\n",
    "        tf.logging.info(acc[-21:])\n",
    "    return acc\n",
    "        \n",
    "def train_a_bunch_of_cifar10s():\n",
    "    wd1_space = np.linspace(0.,0.012,3) # they use 0.004 in the tensorflow example\n",
    "    wd2_space = np.linspace(0.,0.012,3)\n",
    "    wd3_space = np.linspace(0.,0.012,3)\n",
    "    drop1_space = np.linspace(0.2,1,3)\n",
    "    wd_drop_grid = itertools.product(wd1_space, wd2_space, wd3_space, drop1_space)\n",
    "    train_files, test_files, validation_files = maybe_download_and_extract()\n",
    "    try:\n",
    "        with open(\"completed_cifar10.p\",'rb') as f:\n",
    "            completed = pickle.load(f)\n",
    "    except:\n",
    "        completed = []\n",
    "    i = 0\n",
    "    for wd1,wd2,wd3,drop1 in wd_drop_grid:\n",
    "        if i < len(completed): #skip values until we get to a new one\n",
    "            i+=1\n",
    "            continue\n",
    "        acc = train_to_convergence(train_files,wd1,wd2,wd3,drop1)\n",
    "        tf.logging.info(acc[-10:])\n",
    "        completed.append((wd1,wd2,wd3,drop1,acc))\n",
    "        i+=1\n",
    "        with open(\"completed_cifar10.p\",\"wb\") as f:\n",
    "            pickle.dump(completed,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dirs(ROOT.format(DIR))\n",
    "train_a_bunch_of_cifar10s()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
