{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import os\n",
    "import pickle\n",
    "from cifar10_input import *\n",
    "from cifar10_models import *\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "#### SET THIS BEFORE RUNNING\n",
    "MODEL_NAME = \"cifar10/cifar10_scm_001\"\n",
    "DISCRIMINATOR_NAME = \"cifar10/cifar10_scm_001_discriminator_wdropout\"\n",
    "try:\n",
    "    os.makedirs(CHECKPOINT.format(MODEL_NAME))\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "class BundledData(InitializableMixin):\n",
    "    def __init__(self, train_in, train_out, test_in, test_out):\n",
    "        with tf.name_scope(\"data_splitter\"):\n",
    "            self.use_test_data = tf.Variable(0, name=\"input_test_vs_train\", trainable=False)\n",
    "            use_test_data = tf.cast(self.use_test_data, tf.bool)\n",
    "            self.input = tf.cond(use_test_data, lambda: test_in, lambda: train_in)\n",
    "            self.input.set_shape(test_in.shape)\n",
    "            self.output = tf.cond(use_test_data, lambda: test_out, lambda: train_out)\n",
    "            self.output.set_shape(test_out.shape)\n",
    "           \n",
    "        \n",
    "tf.reset_default_graph()\n",
    "train_files, test_files = maybe_download_and_extract()\n",
    "train_data = Cifar10Record(train_files)\n",
    "test_data = Cifar10Record(test_files)\n",
    "data = BundledData(train_data.image, train_data.label, test_data.image, test_data.label)\n",
    "\n",
    "model = Cifar10ShallowConvolutionalModel(data.input, data.output, trainable=False)\n",
    "model = Cifar10ShallowConvolutionalModel(data.input, data.output, trainable=False)\n",
    "\n",
    "try:\n",
    "    checkpoints = sorted([\"{}/{}\".format(ROOT.format(MODEL_NAME),x[:-5]) for x in os.listdir(ROOT.format(MODEL_NAME)) if x.endswith(\".meta\")], \n",
    "                     key=lambda x: int(x.split(\"-\")[-1]))\n",
    "except FileNotFoundError:\n",
    "    checkpoints = []\n",
    "model.saver.recover_last_checkpoints(checkpoints)\n",
    "try: latest_checkpoint = model.saver.last_checkpoints[-4]\n",
    "except: latest_checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(TrainableMixin):\n",
    "    def __init__(self, model, batch_size, summaries=True): \n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self._init_graph(summaries)\n",
    "        self._init_saver()\n",
    "\n",
    "    def _init_graph(self, summaries=True):\n",
    "        with tf.name_scope(\"discriminator\"):\n",
    "            \n",
    "            with tf.name_scope(\"meta_variables\"):\n",
    "                self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "                self.learning_rate = tf.Variable(0.001, name=\"learning_rate\", trainable=False)\n",
    "\n",
    "            with tf.name_scope(\"layer_000_input\"):\n",
    "                self.input_dim = list(np.multiply(2, self.model.output_dim))\n",
    "                self.output_dim = [2]\n",
    "                self.use_test_data = tf.Variable(1, trainable=False)\n",
    "                self.y_ = tf.one_hot(self.use_test_data, 2, on_value=1.0, off_value=0.0)\n",
    "                self.y_ = tf.reshape(self.y_, [1] + self.output_dim)\n",
    "                self.y_ = tf.tile(self.y_,[self.batch_size,1])\n",
    "                self.x = tf.concat((self.model.proba, self.model.y_),1)\n",
    "                layer_000_out_dim = self.input_dim\n",
    "                \n",
    "            with tf.name_scope(\"layer_001_fc\"):\n",
    "                layer_001_in_dim = layer_000_out_dim\n",
    "                layer_001_out_dim = [100]\n",
    "                self.W_001 = tf.Variable(tf.random_normal(layer_001_in_dim + layer_001_out_dim,0,0.1), name=\"weights\")\n",
    "                self.b_001 = tf.Variable(tf.random_normal(layer_001_out_dim,0,0.1), name=\"bias\")\n",
    "                self.logits_001 = tf.add(tf.matmul(self.x,self.W_001, name=\"mul_weights\"), self.b_001, name=\"add_bias\")\n",
    "                self.bent_001 = tf.nn.relu(self.logits_001, name=\"relu\")\n",
    "                self.dropout_rate_001 = tf.Variable(2.0, name=\"dropout_rate\", trainable=False)\n",
    "                self.dropout_001 = tf.nn.dropout(self.bent_001, 1-self.dropout_rate_001, name=\"dropout\")\n",
    "                \n",
    "            with tf.name_scope(\"layer_002_fc\"):\n",
    "                layer_002_in_dim = layer_001_out_dim\n",
    "                layer_002_out_dim = [10]\n",
    "                self.W_002 = tf.Variable(tf.random_normal(layer_002_in_dim + layer_002_out_dim,0,0.1), name=\"weights\")\n",
    "                self.b_002 = tf.Variable(tf.random_normal(layer_002_out_dim,0,0.1), name=\"bias\")\n",
    "                self.logits_002 = tf.add(tf.matmul(self.dropout_001,self.W_002, name=\"mul_weights\"), self.b_002, name=\"add_bias\")\n",
    "                self.bent_002 = tf.nn.relu(self.logits_002, name=\"relu\")\n",
    "                self.dropout_rate_002 = tf.Variable(1.0, name=\"dropout_rate\", trainable=False)\n",
    "                self.dropout_002 = tf.nn.dropout(self.bent_002, 1-self.dropout_rate_002, name=\"dropout\")\n",
    "\n",
    "            with tf.name_scope(\"layer_003_fc\"):\n",
    "                layer_003_in_dim = layer_002_out_dim\n",
    "                layer_003_out_dim = self.output_dim\n",
    "                self.W_003 = tf.Variable(tf.random_normal(layer_003_in_dim + layer_003_out_dim,0,0.1), name=\"weights\")\n",
    "                self.b_003 = tf.Variable(tf.random_normal(layer_003_out_dim,0,0.1), name=\"bias\")\n",
    "                self.logits_003 = tf.add(tf.matmul(self.dropout_002,self.W_003, name=\"mul_weights\"), self.b_003, name=\"add_bias\")\n",
    "                \n",
    "            with tf.name_scope(\"outputs\"):\n",
    "                self.logits = self.logits_003\n",
    "                self.proba = tf.nn.softmax(self.logits)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_, logits=self.logits))\n",
    "                \n",
    "            with tf.name_scope(\"summaries\"):\n",
    "                self.correct_prediction = tf.equal(tf.argmax(self.logits,1), tf.argmax(self.y_,1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32), name=\"accuracy\")\n",
    "                self.summaries = {}\n",
    "                if summaries:\n",
    "                    self.summaries = {\n",
    "                        \"batch acc.\": tf.summary.scalar(\"batch_acc\", self.accuracy)\n",
    "                    }\n",
    "\n",
    "            with tf.name_scope(\"training\"): \n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate, name=\"optimizer\")\n",
    "                self.grads = self.optimizer.compute_gradients(self.loss)\n",
    "                self.grad_application = self.optimizer.apply_gradients(self.grads, global_step = self.global_step)\n",
    "                with tf.control_dependencies([self.grad_application] + list(self.summaries.values())):\n",
    "                    self.train_step = tf.no_op(name=\"train_step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(model, 100)\n",
    "\n",
    "try:\n",
    "    discriminator_checkpoints = sorted([\"{}/{}\".format(ROOT.format(DISCRIMINATOR_NAME),x[:-5]) for x in os.listdir(ROOT.format(DISCRIMINATOR_NAME)) if x.endswith(\".meta\")], \n",
    "                     key=lambda x: int(x.split(\"-\")[-1]))\n",
    "except FileNotFoundError:\n",
    "    discriminator_checkpoints = []\n",
    "discriminator.saver.recover_last_checkpoints(discriminator_checkpoints)\n",
    "try: discriminator_latest_checkpoint = discriminator.saver.last_checkpoints[-2]\n",
    "except: discriminator_latest_checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/cifar10/cifar10_scm_001/model.ckpt-358500\n",
      "500\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_data = Cifar10Record(train_files, epochs=1)\n",
    "test_data = Cifar10Record(test_files, epochs=1)\n",
    "\n",
    "model = Cifar10ShallowConvolutionalModel(train_data.image, train_data.label, trainable=False)\n",
    "model = Cifar10ShallowConvolutionalModel(train_data.image, train_data.label, trainable=False)\n",
    "discriminator = Discriminator(model, 100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model._restore_or_init(sess, latest_checkpoint)\n",
    "    discriminator._restore_or_init(sess, None)\n",
    "    tf.assign(discriminator.dropout_rate_002, 1)\n",
    "    tf.assign(discriminator.dropout_rate_001, 1)\n",
    "    tf.assign(discriminator.use_test_data, 0)\n",
    "    try:\n",
    "        train_acc = []\n",
    "        i=1\n",
    "        while True:\n",
    "            try:\n",
    "                train_acc.append(sess.run(discriminator.accuracy))\n",
    "            except:\n",
    "                break\n",
    "            if not i%500:\n",
    "                print(i)\n",
    "            i+=1\n",
    "        print(np.array(train_acc).mean())\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/cifar10/cifar10_scm_001/model.ckpt-358500\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_data = Cifar10Record(train_files, epochs=1)\n",
    "test_data = Cifar10Record(test_files, epochs=1)\n",
    "\n",
    "model = Cifar10ShallowConvolutionalModel(test_data.image, test_data.label, trainable=False)\n",
    "model = Cifar10ShallowConvolutionalModel(test_data.image, test_data.label, trainable=False)\n",
    "discriminator = Discriminator(model, 100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model._restore_or_init(sess, latest_checkpoint)\n",
    "    discriminator._restore_or_init(sess, None)\n",
    "    tf.assign(discriminator.dropout_rate_002, 1)\n",
    "    tf.assign(discriminator.dropout_rate_001, 1)\n",
    "    tf.assign(discriminator.use_test_data, 0)\n",
    "    try:\n",
    "        test_acc = []\n",
    "        i=1\n",
    "        while True:\n",
    "            try:\n",
    "                test_acc.append(sess.run(discriminator.accuracy))\n",
    "            except:\n",
    "                break\n",
    "            if not i%500:\n",
    "                print(i)\n",
    "            i+=1\n",
    "        print(np.array(test_acc).mean())\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
